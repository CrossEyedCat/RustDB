//! –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ª–µ–∫—Å–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ rustdb

use rustdb::parser::Lexer;

fn main() {
    println!("üîç –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ª–µ–∫—Å–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ rustdb\n");
    
    // –¢–µ—Å—Ç 1: –ë–∞–∑–æ–≤—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
    test_basic_keywords();
    
    // –¢–µ—Å—Ç 2: –°–æ—Å—Ç–∞–≤–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
    test_compound_keywords();
    
    // –¢–µ—Å—Ç 3: –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π SQL –∑–∞–ø—Ä–æ—Å
    test_complex_query();
    
    println!("‚úÖ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!");
}

fn test_basic_keywords() {
    println!("üìù 1. –ë–∞–∑–æ–≤—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞");
    println!("============================");
    
    let sql = "SELECT FROM WHERE INSERT UPDATE DELETE";
    let mut lexer = Lexer::new(sql).unwrap();
    let tokens = lexer.tokenize().unwrap();
    
    for token in &tokens {
        if token.token_type != rustdb::parser::TokenType::Eof {
            println!("   {:?}: '{}'", token.token_type, token.value);
        }
    }
    println!();
}

fn test_compound_keywords() {
    println!("üîó 2. –°–æ—Å—Ç–∞–≤–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞");
    println!("==============================");
    
    let sql = "IS NULL IS NOT NULL NOT NULL GROUP BY ORDER BY INNER JOIN";
    let mut lexer = Lexer::new(sql).unwrap();
    
    loop {
        let token = lexer.next_token().unwrap();
        if token.token_type == rustdb::parser::TokenType::Eof {
            break;
        }
        println!("   {:?}: '{}'", token.token_type, token.value);
    }
    println!();
    
    // –¢–µ—Å—Ç –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–≤
    println!("üîß –¢–µ—Å—Ç –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–≤:");
    let mut lexer2 = rustdb::parser::Lexer::new("+ - * / % = <> < > <= >= != :=").unwrap();
    let tokens = lexer2.tokenize().unwrap();
    
    println!("   –í—Å–µ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤: {}", tokens.len());
    for (i, token) in tokens.iter().enumerate() {
        println!("   {}: {:?} = '{}'", i, token.token_type, token.value);
    }
    println!();
}

fn test_complex_query() {
    println!("üóÉÔ∏è  3. –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π SQL –∑–∞–ø—Ä–æ—Å");
    println!("=============================");
    
    let sql = r#"
        SELECT u.name, u.email, COUNT(o.id) as order_count
        FROM users u
        LEFT JOIN orders o ON u.id = o.user_id
        WHERE u.created_at >= '2023-01-01'
          AND u.status IS NOT NULL
        GROUP BY u.id
        HAVING COUNT(o.id) > 0
        ORDER BY order_count DESC
        LIMIT 10;
    "#;
    
    let mut lexer = Lexer::new(sql).unwrap();
    let tokens = lexer.tokenize().unwrap();
    
    println!("   –í—Å–µ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤: {}", tokens.len());
    
    // –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
    let keywords: Vec<_> = tokens.iter()
        .filter(|t| t.token_type.is_keyword())
        .collect();
    
    println!("   –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞:");
    for token in keywords {
        println!("     {:?}", token.token_type);
    }
    println!();
}
